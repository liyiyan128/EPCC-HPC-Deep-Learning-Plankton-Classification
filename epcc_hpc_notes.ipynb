{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Login\n",
    "```\n",
    "ssh -i ~/.ssh/id_ARCHER2_rsa liyiyan@login.cirrus.ac.uk\n",
    "cd /work/m23ss/m23ss/liyiyan\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Jobs\n",
    "- `sinfo`\n",
    "- `sbatch jobscript.slurm`  // submit a job script containing `srun` commands\n",
    "- `squeue`\n",
    "- `scancel 12345`\n",
    "```\n",
    "srun --nodes=1 --time=00:05:00 --partition=standard --qos=short \\<path to file\\>\n",
    "```\n",
    "\n",
    "#### Interactive Jobs\n",
    "```\n",
    "# partitions = standard or gpu --gres=gpu:X\n",
    "# qos = shrot < 20 mins or standard\n",
    "# --exclusive\n",
    "srun --nodes=1 --time=00:20:00 --partition=gpu --gres=gpu:1 --qos=short --account=m23ss --pty /usr/bin/bash --login\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### conda env\n",
    "```\n",
    "# Create env\n",
    "conda config --prepend envs_dirs ${CONDA_ROOT}/envs\n",
    "conda config --prepend pkgs_dirs ${CONDA_ROOT}/pkgs\n",
    "conda avtivate myenv\n",
    "\n",
    "# Activate env\n",
    "CONDA_ROOT=/work/m23ss/m23ss/liyiyan/condaenvs2\n",
    "    # CONDA_ROOT=/work/m23ss/m23ss/liyiyan/plankton/conda_envs\n",
    "export CONDARC=${CONDA_ROOT}/.condarc\n",
    "eval \"$(conda shell.bash hook)\"\n",
    "\n",
    "conda activate myvenv\n",
    "```\n",
    "\n",
    "#### CPRNet\n",
    "```\n",
    "# cv2\n",
    "conda install -c conda-forge opencv\n",
    "# tqdm\n",
    "conda install -c conda-forge tqdm\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run JupyterLab on Cirrus\n",
    "https://cirrus.readthedocs.io/en/main/user-guide/python.html#using-jupyterlab-on-cirrus\n",
    "```\n",
    "module load anaconda/python3\n",
    "\n",
    "export HOME=/work/m23ss/m23ss/liyiyan\n",
    "export JUPYTER_RUNTIME_DIR=$(pwd)\n",
    "\n",
    "# Start Jupyter server.\n",
    "jupyter notebook --ip=0.0.0.0 --no-browser\n",
    "# --port=5000-65535\n",
    "\n",
    "# Start a new terminal.\n",
    "ssh <username>@login.cirrus.ac.uk -L<port_number>:<node_id>:<port_number>\n",
    "ssh -i ~/.ssh/id_ARCHER2_rsa liyiyan@login.cirrus.ac.uk -L\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "```\n",
    "import torch\n",
    "```\n",
    "\n",
    "- `torch.Tensor` - A multi-dimensional array with support for autograd operations like backward(). Also holds the gradient w.r.t. the tensor.\n",
    "\n",
    "- `nn.Module` - Neural network module. Convenient way of encapsulating parameters, with helpers for moving them to GPU, exporting, loading, etc.\n",
    "\n",
    "- `nn.Parameter` - A kind of Tensor, that is automatically registered as a parameter when assigned as an attribute to a Module.\n",
    "\n",
    "- `autograd.Function` - Implements forward and backward definitions of an autograd operation. Every Tensor operation creates at least a single Function node that connects to functions that created a Tensor and encodes its history.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "- Generalisation of vectors and matrices (multidimensional array).\n",
    "- A tensor ${\\mathcal A} \\isin {\\mathbb F}^{I_{0} \\times ... I_{C}}$, where $I_{i}$ are positive intergers and $C$ is the number of dimensions or mode of the tensor.\n",
    "\n",
    "#### Initialisation\n",
    "```\n",
    "# 1. Directly from data\n",
    "x_data = torch.tensor(data)\n",
    "\n",
    "# 2. From a NumPy array\n",
    "x_np = torch.from_numpy(np_array)\n",
    "\n",
    "# 3. From another tensor\n",
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data (shape, datatype)\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "\n",
    "```\n",
    "#### Attributes\n",
    "`shape`, `dtype`, `device` (device on which they are stored)\n",
    "\n",
    "#### Operations\n",
    "- Element-wise product: `tensor.mul(tensor)` or `tensor * tensor`\n",
    "- Matrix multiplication: `tensor.matmul(tensor.T)` or `tensor @ tensor.T`\n",
    "\n",
    "- In-place operations: operations that have a _ suffix\n",
    "</br>`x.copy_(y)`\n",
    "</br>`x.t_()`\n",
    "</br>`tensor.add_(5)`\n",
    "\n",
    "- Joining: `torch.cat`, `torch.stack`\n",
    "- Tensor to NumPy array: `n = t.numpy()` (Note that a change in the tensor relects in the NumPy array and vice versa)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `torch.autograd`\n",
    "PyTorch's automatic differentiation engine.\n",
    "\n",
    "#### Traning a NN:\n",
    "1. Forward Propagation\n",
    "\n",
    "   In forward prop, the NN makes its best guess about the correct output.\n",
    "   \n",
    "   It runs the input data through each of its functions to make this guess.\n",
    "\n",
    "2. Backward Propagation\n",
    "   \n",
    "   In back prop, the NN adjusts its parameters proportionate to the error in its guess.\n",
    "   \n",
    "   It does this by traversing backwards from the output, collecting the gradients of the error wrt the parameters of the functions, and optimising the parameters using gradient descent.\n",
    "\n",
    "#### E.g. Implemetation\n",
    "\n",
    "```\n",
    "import torch\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "data = torch.rand(1, 3, 64, 64)\n",
    "labels = torch.rand(1, 1000)\n",
    "```\n",
    "\n",
    "1. **Foward pass:**</br>\n",
    "   Run the input data through the model through each of its layers to make a prediction.\n",
    "```\n",
    "prediction = model(data)  # forward pass\n",
    "```\n",
    "\n",
    "2. **Backward pass:**</br>\n",
    "Calculate the error (loss).</br>\n",
    "Backpropagate the error through the network.</br>\n",
    "Call `.backward()` on the error tensor.\n",
    "Autograd then calculates and stores the gradients for each model parameter in the parameter's `.grad` attribute.\n",
    "```\n",
    "loss = (prediction - labels).sum()\n",
    "loss.backward()  # backward pass\n",
    "```\n",
    "\n",
    "3. Load an **optimiser**. Register all the parameters of the model in the optimizer.\n",
    "```\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
    "```\n",
    "\n",
    "4. **Gradient descent:**</br>\n",
    "   Call `.step()` to initiate gradient descent.</br>\n",
    "   The optimiser adjusts each parameter by its gradient stored in `.grad`.\n",
    "```\n",
    "optim.step()  # gradient descent\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "## `torch.nn`\n",
    "\n",
    "A typical training procedure for a neural network:\n",
    "\n",
    "- Define the neural network that has some learnable parameters (or weights)\n",
    "\n",
    "- Iterate over a dataset of inputs\n",
    "\n",
    "- Process input through the network\n",
    "\n",
    "- Compute the loss (how far is the output from being correct)\n",
    "\n",
    "- Propagate gradients back into the networkâ€™s parameters\n",
    "\n",
    "- Update the weights of the network, typically using a simple update rule: `weight = weight - learning_rate * gradient`\n",
    "\n",
    "</br>\n",
    "\n",
    "#### E.g. Implementation\n",
    "\n",
    "1. Define the network:</br>\n",
    "   (The `backward` function is already defined in `autograd`.)\n",
    "```\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square, you can specify with a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "2. Process inputs and call `backward`:\n",
    "```\n",
    "net = Net()\n",
    "\n",
    "input = torch.randn(1, 1, 32, 32)  # random 32x32 input\n",
    "out = net(input)\n",
    "\n",
    "net.zero_grad()  # zero the gradient buffers of all parameters\n",
    "out.backward(torch.randn(1, 10))  # backprop with random gradients\n",
    "\n",
    "params = list(net.parameters())  # learnable parameters\n",
    "```\n",
    "- `torch.nn` only supports mini-batches. The entire torch.nn package only supports inputs that are a mini-batch of samples, and not a single sample.\n",
    "- If you have a single sample, just use `input.unsqueeze(0)` to add a fake batch dimension.\n",
    "\n",
    "3. Compute the loss:</br>\n",
    "    A *loss function* takes (output, target) and computes a value that estimates how far away the output is from the target.</br>\n",
    "    There are several different loss functions under the nn package. A simple loss is `nn.MSELoss` which computes the mean-squared error between the output and the target.\n",
    "```\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make the target the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "```\n",
    "\n",
    "4. Backpropagate the error:</br>\n",
    "   clear the existing gradients and then call `loss.backward`.\n",
    "```\n",
    "net.zero_grad()  # zero the gradient buffers of all parameters\n",
    "\n",
    "loss.backward()\n",
    "```\n",
    "\n",
    "5. Update the weights:</br>\n",
    "   The simplest update rule used in practice is the Stochastic Gradient Descent (SGD): `weight = weight - learning_rate * gradient`.</br>\n",
    "   `torch.optim` implements various update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc.\n",
    "```\n",
    "# SGD\n",
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)\n",
    "\n",
    "\n",
    "# Alternatively, use torch.optim.\n",
    "import torch.optim as optim\n",
    "\n",
    "# Create a optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# In the training loop.\n",
    "optimizer.zero_grad()  # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()  # update\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
